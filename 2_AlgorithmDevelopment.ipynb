{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Music Recommendation Project\n",
    "\n",
    "This is the second section of the Capstone Project for Udacity's Machine Learning Engineer Nanodegree.\n",
    "\n",
    "This notebook includes importing the cleaned data from the first notebook, implementing a baseline algorithm, implementing a complex algorithm, hyper-parameter optimization, and saving the model.\n",
    "\n",
    "Author: Ben Walsh \\\n",
    "February 19, 2021"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contents\n",
    "\n",
    "1. [Feature Import](#feature-data-import)\n",
    "2. Baseline Model\n",
    "3. [Final Model](#xgb-model)\n",
    "4. [Save Model](#save-model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a class=\"anchor\" id=\"feature-data-import\"></a>1. Feature Data Import"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import pickle\n",
    "\n",
    "import os\n",
    "import json\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First import all cleaned feature data: X and y target data for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_file = './data-input-clean/X_train.csv'\n",
    "y_train_file = './data-input-clean/y_train.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(X_train_file):\n",
    "    X_train = pd.read_csv(X_train_file)\n",
    "else:\n",
    "    print('Training data file {} not found!'.format(X_train_file))\n",
    "\n",
    "if os.path.exists(y_train_file):\n",
    "    y_train = pd.read_csv(y_train_file)\n",
    "else:\n",
    "    print('Training data file {} not found!'.format(y_train_file))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "baseline_model = LogisticRegression(random_state=0).fit(X=X_train.values, y=y_train.values.reshape(-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save baseline model with pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_folder = './saved_models'\n",
    "if not(os.path.exists(model_folder)):\n",
    "       os.mkdir(model_folder)\n",
    "\n",
    "pickle.dump(baseline_model, open('{}/baseline-model'.format(model_folder), \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a class=\"anchor\" id=\"xgb-model\"></a>3. XGBoost Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_hparams = {'objective': 'binary:logistic',\n",
    "                   'colsample_bytree': 0.3,\n",
    "                   'learning_rate': 0.1,\n",
    "                   'max_depth': 12, \n",
    "                   'min_child_weight': 1,\n",
    "                   'alpha': 2, # regularization parameter - the higher, the more conservative\n",
    "                   'n_estimators': 50}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_model = xgb.XGBRegressor(objective = xgb_hparams['objective'], \n",
    "                             colsample_bytree = xgb_hparams['colsample_bytree'], \n",
    "                             learning_rate = xgb_hparams['learning_rate'],\n",
    "                             max_depth = xgb_hparams['max_depth'], \n",
    "                             min_child_weight = xgb_hparams['min_child_weight'], \n",
    "                             alpha = xgb_hparams['alpha'], \n",
    "                             n_estimators = xgb_hparams['n_estimators'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20:58:47] WARNING: C:\\Jenkins\\workspace\\xgboost-win64_release_0.90\\src\\learner.cc:686: Tree method is automatically selected to be 'approx' for faster speed. To use old behavior (exact greedy algorithm on single machine), set tree_method to 'exact'.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "XGBRegressor(alpha=2, colsample_bytree=0.3, max_depth=12, n_estimators=50,\n",
       "             objective='binary:logistic')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyper-parameter Optimization\n",
    "Will probably have to redo this every time the input feature data changes. First find the overall structure with max_depth and min_child_weight. A lower max_depth and higher min_child_weight will favor a simpler tree structure, less prone to overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# revisit this later\n",
    "hyper_param_tune = False\n",
    "if hyper_param_tune:\n",
    "    param_test1 = {\n",
    "     'max_depth':range(10,13,2),\n",
    "     'min_child_weight':range(1,4,2)\n",
    "    }\n",
    "    gsearch1 = GridSearchCV(estimator = \\\n",
    "                            XGBClassifier( learning_rate = xgb_hparams['learning_rate'], \n",
    "                                          n_estimators = xgb_hparams['n_estimators'], \n",
    "                                          max_depth = xgb_hparams['max_depth'],\n",
    "                                         min_child_weight = xgb_hparams['min_child_weight'], \n",
    "                                          alpha = xgb_hparams['alpha'],\n",
    "                                         objective = xgb_hparams['objective'], \n",
    "                                          nthread = 2, \n",
    "                                          seed = 62), \n",
    "    param_grid = param_test1, scoring='roc_auc',n_jobs=2,iid=False, cv=5)\n",
    "    gsearch1.fit(X_train, y_train)\n",
    "    gsearch1.grid_scores_, gsearch1.best_params_, gsearch1.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a class=\"anchor\" id=\"save-model\"></a>4. Save Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get timestamp for history and to ensure a unique model name. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamp = datetime.datetime.now()\n",
    "timestamp_str = '{}-{:02}-{:02}-{:02}-{}-{}-{}'.format(timestamp.year, timestamp.month, timestamp.day, timestamp.hour, timestamp.minute, timestamp.second, timestamp.microsecond)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save model with pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(xgb_model, open('{}/model-{}'.format(model_folder, timestamp_str), \"wb\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
